/data/hzy/lvm4-ts/IMTS/run_models.py
2025-04-27 13:10:13
run_models.py --model VisionTS-B --dataset mimic --state def --history 24 --patience 15 --batch_size 12 --lr 1e-4 --patch_size 8 --stride 8 --nhead 1 --tf_layer 1 --nlayer 3 --te_dim 40 --node_dim 40 --hid_dim 40 --mask_ratio 0.4 --train_mode fine-tune --finetune_type wo_ssl_ft --pretrain_weights_dir /data/hzy/lvm4-ts/IMTS/logs/mimic/mimic_Hyper_hiddim_pretrain/pretrain_weights/Exp53713_VisionTS-B_hid_dim40_te_dim40_node_dim40_mask_ratio0.4_stride8.0_patchsize8.0_seed1_nlayer3_best.pth --seed 2 --gpu 1 --log_dir mimic_Hyper_sota_finetune
Namespace(state='def', n=100000000, hop=1, nhead=1, tf_layer=1, nlayer=3, epoch=1000, patience=15, history=24, patch_size=8.0, stride=8.0, logmode='a', lr=0.0001, w_decay=0.0, batch_size=12, load=None, seed=2, dataset='mimic', log_dir='mimic_Hyper_sota_finetune', pretrain_weights_dir='/data/hzy/lvm4-ts/IMTS/logs/mimic/mimic_Hyper_hiddim_pretrain/pretrain_weights/Exp53713_VisionTS-B_hid_dim40_te_dim40_node_dim40_mask_ratio0.4_stride8.0_patchsize8.0_seed1_nlayer3_best.pth', quantization=0.0, model='VisionTS-B', mask_flag=False, hid_dim=40, te_dim=40, node_dim=40, gpu='1', log_suffix='', periodicity=None, mask_ratio=0.4, train_mode='fine-tune', TTT=False, ttt_iter=None, ttt_lr=None, apply_lora=False, lora_r=8, lora_alpha=1, lora_dropout=0.0, merge_weights=False, finetune_type='wo_ssl_ft', encoder_only=False, few_shot_ratio=None, npatch=3, world_size=1, device=device(type='cuda'), ft_type='full', arch='mae_base', vm_pretrained=True, vm_ckpt='./ckpt/', interpolation='bilinear', norm_const=0.4, align_const=0.4, grid_method='linear', if_patch=True, pred_window=24, npred_patch=3, ndim=96)
- Epoch 000, ExpID 20089
Train - Loss (one batch): 0.00615
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01708, 0.01708, 0.13070, 0.07276, 129.72%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 0, 0.01556, 0.01556, 0.12474, 0.06960, 99.77%
Time spent: 456.35s
- Epoch 001, ExpID 20089
Train - Loss (one batch): 0.01100
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01704, 0.01704, 0.13053, 0.07218, 128.67%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.01522, 0.01522, 0.12338, 0.06832, 98.38%
Time spent: 456.36s
- Epoch 002, ExpID 20089
Train - Loss (one batch): 0.00496
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01778, 0.01778, 0.13336, 0.07663, 140.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 1, 0.01522, 0.01522, 0.12338, 0.06832, 98.38%
Time spent: 414.52s
- Epoch 003, ExpID 20089
Train - Loss (one batch): 0.02035
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01660, 0.01660, 0.12884, 0.07124, 116.52%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 3, 0.01507, 0.01507, 0.12275, 0.06793, 89.42%
Time spent: 454.69s
- Epoch 004, ExpID 20089
Train - Loss (one batch): 0.01632
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01639, 0.01639, 0.12804, 0.07072, 118.87%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.01482, 0.01482, 0.12172, 0.06728, 91.00%
Time spent: 454.99s
- Epoch 005, ExpID 20089
Train - Loss (one batch): 0.00761
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01669, 0.01669, 0.12920, 0.07293, 134.61%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.01482, 0.01482, 0.12172, 0.06728, 91.00%
Time spent: 418.07s
- Epoch 006, ExpID 20089
Train - Loss (one batch): 0.01323
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01660, 0.01660, 0.12884, 0.06980, 112.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 4, 0.01482, 0.01482, 0.12172, 0.06728, 91.00%
Time spent: 422.91s
- Epoch 007, ExpID 20089
Train - Loss (one batch): 0.01415
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01627, 0.01627, 0.12755, 0.07065, 111.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.01471, 0.01471, 0.12127, 0.06760, 86.11%
Time spent: 454.29s
- Epoch 008, ExpID 20089
Train - Loss (one batch): 0.00674
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01640, 0.01640, 0.12807, 0.06978, 111.92%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.01471, 0.01471, 0.12127, 0.06760, 86.11%
Time spent: 419.10s
- Epoch 009, ExpID 20089
Train - Loss (one batch): 0.01254
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01632, 0.01632, 0.12777, 0.07019, 105.89%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 7, 0.01471, 0.01471, 0.12127, 0.06760, 86.11%
Time spent: 419.48s
- Epoch 010, ExpID 20089
Train - Loss (one batch): 0.01031
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01608, 0.01608, 0.12679, 0.07018, 118.82%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 10, 0.01444, 0.01444, 0.12018, 0.06661, 93.39%
Time spent: 456.88s
- Epoch 011, ExpID 20089
Train - Loss (one batch): 0.01097
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01631, 0.01631, 0.12772, 0.07336, 130.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 10, 0.01444, 0.01444, 0.12018, 0.06661, 93.39%
Time spent: 423.04s
- Epoch 012, ExpID 20089
Train - Loss (one batch): 0.02070
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01585, 0.01585, 0.12591, 0.07028, 112.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 12, 0.01427, 0.01427, 0.11946, 0.06683, 88.26%
Time spent: 456.73s
- Epoch 013, ExpID 20089
Train - Loss (one batch): 0.00641
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01582, 0.01582, 0.12578, 0.06848, 115.31%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01417, 0.01417, 0.11905, 0.06477, 89.17%
Time spent: 452.83s
- Epoch 014, ExpID 20089
Train - Loss (one batch): 0.02860
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01597, 0.01597, 0.12636, 0.07062, 120.37%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01417, 0.01417, 0.11905, 0.06477, 89.17%
Time spent: 420.44s
- Epoch 015, ExpID 20089
Train - Loss (one batch): 0.01205
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01587, 0.01587, 0.12596, 0.06990, 103.00%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 13, 0.01417, 0.01417, 0.11905, 0.06477, 89.17%
Time spent: 419.52s
- Epoch 016, ExpID 20089
Train - Loss (one batch): 0.00773
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01574, 0.01574, 0.12544, 0.06867, 104.42%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01423, 0.01423, 0.11930, 0.06522, 80.26%
Time spent: 454.32s
- Epoch 017, ExpID 20089
Train - Loss (one batch): 0.01101
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01579, 0.01579, 0.12567, 0.07060, 124.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 16, 0.01423, 0.01423, 0.11930, 0.06522, 80.26%
Time spent: 419.72s
- Epoch 018, ExpID 20089
Train - Loss (one batch): 0.00860
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01557, 0.01557, 0.12477, 0.06852, 117.14%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01405, 0.01405, 0.11852, 0.06524, 91.19%
Time spent: 454.79s
- Epoch 019, ExpID 20089
Train - Loss (one batch): 0.01311
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01637, 0.01637, 0.12793, 0.07216, 115.05%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01405, 0.01405, 0.11852, 0.06524, 91.19%
Time spent: 415.91s
- Epoch 020, ExpID 20089
Train - Loss (one batch): 0.00421
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01606, 0.01606, 0.12673, 0.06915, 106.73%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01405, 0.01405, 0.11852, 0.06524, 91.19%
Time spent: 417.66s
- Epoch 021, ExpID 20089
Train - Loss (one batch): 0.00734
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01578, 0.01578, 0.12563, 0.07032, 123.29%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 18, 0.01405, 0.01405, 0.11852, 0.06524, 91.19%
Time spent: 420.08s
- Epoch 022, ExpID 20089
Train - Loss (one batch): 0.02158
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01555, 0.01555, 0.12468, 0.06855, 116.38%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 22, 0.01402, 0.01402, 0.11842, 0.06526, 92.38%
Time spent: 452.52s
- Epoch 023, ExpID 20089
Train - Loss (one batch): 0.00832
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01592, 0.01592, 0.12617, 0.07005, 111.44%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 22, 0.01402, 0.01402, 0.11842, 0.06526, 92.38%
Time spent: 418.72s
- Epoch 024, ExpID 20089
Train - Loss (one batch): 0.01197
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01662, 0.01662, 0.12890, 0.06915, 108.43%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 22, 0.01402, 0.01402, 0.11842, 0.06526, 92.38%
Time spent: 419.59s
- Epoch 025, ExpID 20089
Train - Loss (one batch): 0.00471
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01553, 0.01553, 0.12461, 0.06850, 109.21%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01393, 0.01393, 0.11804, 0.06501, 88.26%
Time spent: 454.56s
- Epoch 026, ExpID 20089
Train - Loss (one batch): 0.00854
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01580, 0.01580, 0.12572, 0.06957, 114.96%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01393, 0.01393, 0.11804, 0.06501, 88.26%
Time spent: 421.08s
- Epoch 027, ExpID 20089
Train - Loss (one batch): 0.00530
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01595, 0.01595, 0.12631, 0.07001, 119.10%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01393, 0.01393, 0.11804, 0.06501, 88.26%
Time spent: 423.35s
- Epoch 028, ExpID 20089
Train - Loss (one batch): 0.00896
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01570, 0.01570, 0.12530, 0.06818, 106.34%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01393, 0.01393, 0.11804, 0.06501, 88.26%
Time spent: 418.88s
- Epoch 029, ExpID 20089
Train - Loss (one batch): 0.01824
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01577, 0.01577, 0.12556, 0.06815, 111.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01393, 0.01393, 0.11804, 0.06501, 88.26%
Time spent: 419.88s
- Epoch 030, ExpID 20089
Train - Loss (one batch): 0.00995
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01572, 0.01572, 0.12538, 0.06905, 108.20%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01393, 0.01393, 0.11804, 0.06501, 88.26%
Time spent: 418.96s
- Epoch 031, ExpID 20089
Train - Loss (one batch): 0.01138
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01586, 0.01586, 0.12592, 0.06893, 108.03%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01393, 0.01393, 0.11804, 0.06501, 88.26%
Time spent: 418.87s
- Epoch 032, ExpID 20089
Train - Loss (one batch): 0.01178
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01615, 0.01615, 0.12708, 0.07271, 114.56%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 25, 0.01393, 0.01393, 0.11804, 0.06501, 88.26%
Time spent: 421.08s
- Epoch 033, ExpID 20089
Train - Loss (one batch): 0.01571
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01541, 0.01541, 0.12414, 0.06901, 101.19%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 453.69s
- Epoch 034, ExpID 20089
Train - Loss (one batch): 0.00744
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01550, 0.01550, 0.12449, 0.06770, 102.26%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 420.22s
- Epoch 035, ExpID 20089
Train - Loss (one batch): 0.00891
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01548, 0.01548, 0.12441, 0.06827, 112.86%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 419.33s
- Epoch 036, ExpID 20089
Train - Loss (one batch): 0.01676
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01583, 0.01583, 0.12583, 0.06897, 106.90%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 419.26s
- Epoch 037, ExpID 20089
Train - Loss (one batch): 0.01002
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01599, 0.01599, 0.12646, 0.07077, 107.24%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 418.53s
- Epoch 038, ExpID 20089
Train - Loss (one batch): 0.00562
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01581, 0.01581, 0.12575, 0.06785, 107.57%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 418.79s
- Epoch 039, ExpID 20089
Train - Loss (one batch): 0.00947
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01603, 0.01603, 0.12661, 0.07005, 120.57%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 420.72s
- Epoch 040, ExpID 20089
Train - Loss (one batch): 0.01384
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01572, 0.01572, 0.12537, 0.06811, 105.13%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 414.95s
- Epoch 041, ExpID 20089
Train - Loss (one batch): 0.01249
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01569, 0.01569, 0.12525, 0.06827, 101.49%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 419.87s
- Epoch 042, ExpID 20089
Train - Loss (one batch): 0.01179
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01567, 0.01567, 0.12517, 0.06997, 105.85%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 415.93s
- Epoch 043, ExpID 20089
Train - Loss (one batch): 0.00918
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01573, 0.01573, 0.12540, 0.06867, 103.53%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 408.95s
- Epoch 044, ExpID 20089
Train - Loss (one batch): 0.01227
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01548, 0.01548, 0.12442, 0.06800, 109.15%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 412.25s
- Epoch 045, ExpID 20089
Train - Loss (one batch): 0.01172
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01608, 0.01608, 0.12682, 0.07297, 121.58%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 411.25s
- Epoch 046, ExpID 20089
Train - Loss (one batch): 0.01187
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01595, 0.01595, 0.12629, 0.06980, 104.64%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 405.22s
- Epoch 047, ExpID 20089
Train - Loss (one batch): 0.00679
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01580, 0.01580, 0.12570, 0.06860, 104.71%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 432.06s
- Epoch 048, ExpID 20089
Train - Loss (one batch): 0.00658
Val - Loss, MSE, RMSE, MAE, MAPE: 0.01579, 0.01579, 0.12564, 0.06978, 106.16%
Test - Best epoch, Loss, MSE, RMSE, MAE, MAPE: 33, 0.01379, 0.01379, 0.11744, 0.06562, 81.04%
Time spent: 431.38s
